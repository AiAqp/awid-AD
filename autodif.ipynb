{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward AD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dual{Float64}[Dual(1, 2), Dual(3, 0)] = Dual{Float64}[(1.0) + [2.0ϵ], (3.0) + [0.0ϵ]]\n",
      "Dual{Float64}[1, 2, 3] = Dual{Float64}[(1.0) + [0.0ϵ], (2.0) + [0.0ϵ], (3.0) + [0.0ϵ]]\n",
      "Dual(1, 2) * 3 = (3) + [6ϵ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "jacobian (generic function with 1 method)"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "struct Dual{T <:Number} <:Number\n",
    "    v::T\n",
    "   dv::T\n",
    "end\n",
    "\n",
    "import Base: +, -, *, /\n",
    "-(x::Dual)          = Dual(-x.v,       -x.dv)\n",
    "+(x::Dual, y::Dual) = Dual( x.v + y.v,  x.dv + y.dv)\n",
    "-(x::Dual, y::Dual) = Dual( x.v - y.v,  x.dv - y.dv)\n",
    "*(x::Dual, y::Dual) = Dual( x.v * y.v,  x.dv * y.v + x.v * y.dv)\n",
    "/(x::Dual, y::Dual) = Dual( x.v / y.v, (x.dv * y.v - x.v * y.dv)/y.v^2)\n",
    "\n",
    "import Base: abs, sin, cos, tan, exp, sqrt, isless\n",
    "abs(x::Dual)  = Dual(abs(x.v),sign(x.v)*x.dv)\n",
    "sin(x::Dual)  = Dual(sin(x.v), cos(x.v)*x.dv)\n",
    "cos(x::Dual)  = Dual(cos(x.v),-sin(x.v)*x.dv)\n",
    "tan(x::Dual)  = Dual(tan(x.v), one(x.v)*x.dv + tan(x.v)^2*x.dv)\n",
    "exp(x::Dual)  = Dual(exp(x.v), exp(x.v)*x.dv)\n",
    "sqrt(x::Dual) = Dual(sqrt(x.v),.5/sqrt(x.v) * x.dv)\n",
    "isless(x::Dual, y::Dual) = x.v < y.v;\n",
    "\n",
    "import Base: convert, promote_rule\n",
    "\n",
    "convert(::Type{Dual{T}}, x::Dual) where T = Dual(convert(T, x.v), convert(T, x.dv))\n",
    "@show Dual{Float64}[Dual(1,2), Dual(3,0)];\n",
    "\n",
    "convert(::Type{Dual{T}}, x::Number) where T = Dual(convert(T, x), zero(T))\n",
    "@show Dual{Float64}[1, 2, 3];\n",
    "\n",
    "promote_rule(::Type{Dual{T}}, ::Type{R}) where {T,R} = Dual{promote_type(T,R)}\n",
    "@show Dual(1,2) * 3;\n",
    "\n",
    "import Base: show\n",
    "show(io::IO, x::Dual) = print(io, \"(\", x.v, \") + [\", x.dv, \"ϵ]\");\n",
    "value(x::Dual) = x.v;\n",
    "partials(x::Dual) = x.dv;\n",
    "\n",
    "J = function jacobian(f, args::Vector{T}) where {T <:Number}\n",
    "    jacobian_columns = Matrix{T}[]\n",
    "    \n",
    "    for i=1:length(args)\n",
    "        x = Dual{T}[]\n",
    "        for j=1:length(args)\n",
    "            seed = (i == j)\n",
    "            push!(x, seed ?\n",
    "                Dual(args[j], one(args[j])) :\n",
    "                Dual(args[j],zero(args[j])) )\n",
    "        end\n",
    "        column = partials.([f(x)...])\n",
    "        push!(jacobian_columns, column[:,:])\n",
    "    end\n",
    "    hcat(jacobian_columns...)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward! (generic function with 4 methods)"
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract type GraphNode end\n",
    "abstract type Operator <: GraphNode end\n",
    "\n",
    "struct Constant{T} <: GraphNode\n",
    "    output :: T\n",
    "end\n",
    "\n",
    "mutable struct Variable <: GraphNode\n",
    "    output :: Any\n",
    "    gradient :: Any\n",
    "    name :: String\n",
    "    Variable(output; name=\"?\") = new(output, nothing, name)\n",
    "end\n",
    "\n",
    "mutable struct ScalarOperator{F} <: Operator\n",
    "    inputs :: Any\n",
    "    output :: Any\n",
    "    gradient :: Any\n",
    "    name :: String\n",
    "    ScalarOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, nothing, nothing, name)\n",
    "end\n",
    "\n",
    "mutable struct BroadcastedOperator{F} <: Operator\n",
    "    inputs :: Any\n",
    "    output :: Any\n",
    "    gradient :: Any\n",
    "    name :: String\n",
    "    BroadcastedOperator(fun, inputs...; name=\"?\") = new{typeof(fun)}(inputs, nothing, nothing, name)\n",
    "end\n",
    "\n",
    "import Base: show, summary\n",
    "show(io::IO, x::ScalarOperator{F}) where {F} = print(io, \"op \", x.name, \"(\", F, \")\");\n",
    "show(io::IO, x::BroadcastedOperator{F}) where {F} = print(io, \"op.\", x.name, \"(\", F, \")\");\n",
    "show(io::IO, x::Constant) = print(io, \"const \", x.output)\n",
    "show(io::IO, x::Variable) = begin\n",
    "    print(io, \"var \", x.name);\n",
    "    print(io, \"\\n ┣━ ^ \"); summary(io, x.output)\n",
    "    print(io, \"\\n ┗━ ∇ \");  summary(io, x.gradient)\n",
    "end\n",
    "\n",
    "function visit(node::GraphNode, visited, order)\n",
    "    if node ∈ visited\n",
    "    else\n",
    "        push!(visited, node)\n",
    "        push!(order, node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "    \n",
    "function visit(node::Operator, visited, order)\n",
    "    if node ∈ visited\n",
    "    else\n",
    "        push!(visited, node)\n",
    "        for input in node.inputs\n",
    "            visit(input, visited, order)\n",
    "        end\n",
    "        push!(order, node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function topological_sort(head::GraphNode)\n",
    "    visited = Set()\n",
    "    order = Vector()\n",
    "    visit(head, visited, order)\n",
    "    return order\n",
    "end\n",
    "\n",
    "reset!(node::Constant) = nothing\n",
    "reset!(node::Variable) = node.gradient = nothing\n",
    "reset!(node::Operator) = node.gradient = nothing\n",
    "\n",
    "compute!(node::Constant) = nothing\n",
    "compute!(node::Variable) = nothing\n",
    "compute!(node::Operator) =\n",
    "    node.output = forward(node, [input.output for input in node.inputs]...)\n",
    "\n",
    "function forward!(order::Vector)\n",
    "    for node in order\n",
    "        compute!(node)\n",
    "        reset!(node)\n",
    "    end\n",
    "    return last(order).output\n",
    "end\n",
    "\n",
    "update!(node::Constant, gradient) = nothing\n",
    "update!(node::GraphNode, gradient) = if isnothing(node.gradient)\n",
    "    node.gradient = gradient else node.gradient .+= gradient\n",
    "end\n",
    "\n",
    "function backward!(order::Vector; seed=1.0)\n",
    "    result = last(order)\n",
    "    result.gradient = seed\n",
    "    @assert length(result.output) == 1 \"Gradient is defined only for scalar functions\"\n",
    "    for node in reverse(order)\n",
    "        backward!(node)\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "function backward!(node::Constant) end\n",
    "function backward!(node::Variable) end\n",
    "function backward!(node::Operator)\n",
    "    inputs = node.inputs\n",
    "    gradients = backward(node, [input.output for input in inputs]..., node.gradient)\n",
    "    for (input, gradient) in zip(inputs, gradients)\n",
    "        update!(input, gradient)\n",
    "    end\n",
    "    return nothing\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "backward (generic function with 13 methods)"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import Base: ^\n",
    "^(x::GraphNode, n::GraphNode) = ScalarOperator(^, x, n)\n",
    "forward(::ScalarOperator{typeof(^)}, x, n) = return x^n\n",
    "backward(::ScalarOperator{typeof(^)}, x, n, g) = tuple(g * n * x ^ (n-1), g * log(abs(x)) * x ^ n)\n",
    "\n",
    "import Base: sin\n",
    "sin(x::GraphNode) = ScalarOperator(sin, x)\n",
    "forward(::ScalarOperator{typeof(sin)}, x) = return sin(x)\n",
    "backward(::ScalarOperator{typeof(sin)}, x, g) = tuple(g * cos(x))\n",
    "\n",
    "import Base: *\n",
    "import LinearAlgebra: mul!\n",
    "*(A::GraphNode, x::GraphNode) = BroadcastedOperator(mul!, A, x)\n",
    "forward(::BroadcastedOperator{typeof(mul!)}, A, x) = return A * x\n",
    "backward(::BroadcastedOperator{typeof(mul!)}, A, x, g) = tuple(g * x', A' * g)\n",
    "\n",
    "Base.Broadcast.broadcasted(*, x::GraphNode, y::GraphNode) = BroadcastedOperator(*, x, y)\n",
    "forward(::BroadcastedOperator{typeof(*)}, x, y) = return x .* y\n",
    "backward(node::BroadcastedOperator{typeof(*)}, x, y, g) = let\n",
    "    𝟏 = ones(length(node.output))\n",
    "    Jx = diagm(y .* 𝟏)\n",
    "    Jy = diagm(x .* 𝟏)\n",
    "    tuple(Jx' * g, Jy' * g)\n",
    "end\n",
    "\n",
    "Base.Broadcast.broadcasted(-, x::GraphNode, y::GraphNode) = BroadcastedOperator(-, x, y)\n",
    "forward(::BroadcastedOperator{typeof(-)}, x, y) = return x .- y\n",
    "backward(::BroadcastedOperator{typeof(-)}, x, y, g) = tuple(g,-g)\n",
    "\n",
    "Base.Broadcast.broadcasted(-, x::GraphNode) = BroadcastedOperator(-, x)\n",
    "forward(::BroadcastedOperator{typeof(-)}, x) = return -x\n",
    "backward(::BroadcastedOperator{typeof(-)}, x, g) = tuple(-g)\n",
    "\n",
    "Base.Broadcast.broadcasted(+, x::GraphNode, y::GraphNode) = BroadcastedOperator(+, x, y)\n",
    "forward(::BroadcastedOperator{typeof(+)}, x, y) = return x .+ y\n",
    "backward(::BroadcastedOperator{typeof(+)}, x, y, g) = tuple(g, g)\n",
    "\n",
    "import Base: sum\n",
    "sum(x::GraphNode) = BroadcastedOperator(sum, x)\n",
    "forward(::BroadcastedOperator{typeof(sum)}, x) = return sum(x)\n",
    "backward(::BroadcastedOperator{typeof(sum)}, x, g) = let\n",
    "    𝟏 = ones(length(x))\n",
    "    J = 𝟏'\n",
    "    tuple(J' * g)\n",
    "end\n",
    "\n",
    "Base.Broadcast.broadcasted(/, x::GraphNode, y::GraphNode) = BroadcastedOperator(/, x, y)\n",
    "forward(::BroadcastedOperator{typeof(/)}, x, y) = return x ./ y\n",
    "backward(node::BroadcastedOperator{typeof(/)}, x, y, g) = let\n",
    "    𝟏 = ones(length(node.output))\n",
    "    Jx = diagm(𝟏 ./ y)\n",
    "    Jy = (-x ./ y .^2)\n",
    "    tuple(Jx' * g, Jy' * g)\n",
    "end\n",
    "\n",
    "import Base: max\n",
    "Base.Broadcast.broadcasted(max, x::GraphNode, y::GraphNode) = BroadcastedOperator(max, x, y)\n",
    "forward(::BroadcastedOperator{typeof(max)}, x, y) = return max.(x, y)\n",
    "backward(::BroadcastedOperator{typeof(max)}, x, y, g) = let\n",
    "    Jx = diagm(isless.(y, x))\n",
    "    Jy = diagm(isless.(x, y))\n",
    "    tuple(Jx' * g, Jy' * g)\n",
    "end\n",
    "\n",
    "import Base: min\n",
    "Base.Broadcast.broadcasted(min, x::GraphNode, y::GraphNode) = BroadcastedOperator(min, x, y)\n",
    "forward(::BroadcastedOperator{typeof(min)}, x, y) = return min.(x, y)\n",
    "backward(::BroadcastedOperator{typeof(min)}, x, y, g) = let\n",
    "    Jx = diagm(isless.(x, y))\n",
    "    Jy = diagm(isless.(y, x))\n",
    "    tuple(Jx' * g, Jy' * g)\n",
    "end\n",
    "\n",
    "import Base: exp \n",
    "Base.Broadcast.broadcasted(exp, x::GraphNode) = BroadcastedOperator(exp, x)\n",
    "forward(::BroadcastedOperator{typeof(exp)}, x) = return exp.(x)\n",
    "backward(::BroadcastedOperator{typeof(exp)}, x, g) = let\n",
    "    tuple(exp.(x) .* g)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ImageCore\n",
    "using MLDatasets\n",
    "MNIST.convert2image(MNIST.traintensor(1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "function vdigit(y::Int)\n",
    "    yv = zeros(10)\n",
    "    yv[y+1] = 1\n",
    "    yv\n",
    "end\n",
    "\n",
    "# Forward mode AD\n",
    "ReLU_f(x) = max(zero(x), x)\n",
    "ReLU2_f(x) = max(zero(x), x) + min(zero(x),x)*0.01\n",
    "σ_f(x) = one(x) / (one(x) + exp(-x))\n",
    "\n",
    "mse_f(y::Vector, ŷ::Vector) = sum(0.5(y - ŷ).^2)\n",
    "\n",
    "fullyconnected(w::Vector, n::Number, m::Number, v::Vector, activation::Function) = activation.(reshape(w, n, m) * v)\n",
    "Wh_f  = randn(15,784)\n",
    "Wo_f  = randn(10,15)\n",
    "dWh_f = similar(Wh_f)\n",
    "dWo_f = similar(Wo_f)\n",
    "x_f = reshape(MNIST.traintensor(Float64,1),784)\n",
    "y_f = vdigit(MNIST.trainlabels(1))\n",
    "e_f = Float64[]\n",
    "\n",
    "function net_f(x, wh, wo, y)\n",
    "    a = fullyconnected(wh, 15, 784, x, ReLU2_f)\n",
    "    ŷ = fullyconnected(wo, 10, 15, a, u->u)\n",
    "    E = mse_f(y, ŷ)\n",
    "end\n",
    "\n",
    "dnet_Wh(x, wh, wo, y) = J(w -> net_f(x, w, wo, y), wh);\n",
    "\n",
    "dnet_Wo(x, wh, wo, y) = J(w -> net_f(x, wh, w, y), wo);\n",
    "\n",
    "# Reverse mode AD\n",
    "function ReLU_r(x) \n",
    "    return max.(Constant(zeros(15)), x) .+ min.(Constant(zeros(15)), x).*Constant(0.01)\n",
    "end\n",
    "\n",
    "function σ_r(x)\n",
    "    return Constant(ones(15)) ./ (Constant(ones(15)) .+ exp.(.-x))\n",
    "end\n",
    "\n",
    "Wh_r  = Variable(Wh_f, name=\"wh\")\n",
    "Wo_r  = Variable(Wo_f, name=\"wo\")\n",
    "x_r = Variable(x_f, name=\"x\")\n",
    "y_r = Variable(y_f, name=\"y\")\n",
    "e_r = Float64[]\n",
    "\n",
    "function dense(w, b, x, activation) return activation(w * x .+ b) end\n",
    "function dense(w, x, activation) return activation(w * x) end\n",
    "function dense(w, x) return w * x end\n",
    "\n",
    "function mse_r(y, ŷ)\n",
    "    return sum(Constant(0.5) .* (y .- ŷ) .* (y .- ŷ))\n",
    "end\n",
    "\n",
    "function net_r(x, wh, wo, y)\n",
    "    x̂ = dense(wh, x, ReLU_r)\n",
    "    x̂.name = \"x̂\"\n",
    "    ŷ = dense(wo, x̂)\n",
    "    ŷ.name = \"ŷ\"\n",
    "    E = mse_r(y, ŷ)\n",
    "    E.name = \"loss\"\n",
    "    return topological_sort(E)\n",
    "end\n",
    "\n",
    "graph = net_r(x_r, Wh_r, Wo_r, y_r);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "step_forward (generic function with 1 method)"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gradient + momentum step\n",
    "α = 0.1\n",
    "β = 0.5\n",
    "vh_r = zero(Wh_r.output)\n",
    "vo_r = zero(Wo_r.output)\n",
    "vh_f = zero(Wh_f)\n",
    "vo_f = zero(Wo_f)\n",
    "\n",
    "function step_reverse()\n",
    "    e = forward!(graph)\n",
    "    backward!(graph)\n",
    "    vh_r[:] = β*vh_r .- α*Wh_r.gradient;\n",
    "    Wh_r.output += vh_r;\n",
    "    vo_r[:] = β*vo_r .- α*Wo_r.gradient;\n",
    "    Wo_r.output += vo_r;\n",
    "    return e\n",
    "end\n",
    "\n",
    "function step_forward()\n",
    "    e = net_f(x_f, Wh_f[:], Wo_f[:], y_f);\n",
    "    # dnet_Wh(x, wh, wo, y) = J(w -> net_f(x, w, wo, y), wh);\n",
    "    # dnet_Wo(x, wh, wo, y) = J(w -> net_f(x, wh, w, y), wo);\n",
    "    dWh_f[:] = dnet_Wh(x_f, Wh_f[:], Wo_f[:], y_f);\n",
    "    dWo_f[:] = dnet_Wo(x_f, Wh_f[:], Wo_f[:], y_f);\n",
    "    vh_f[:] = β*vh_f .- α*dWh_f;\n",
    "    Wh_f[:] += vh_f[:];\n",
    "    vo_f[:] = β*vo_r .- α*dWo_f;\n",
    "    Wo_f[:] += vo_r[:];\n",
    "    return e\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " 676.3729930074519\n",
       "   8.283530501772796\n",
       "   2.8080078728759323"
      ]
     },
     "execution_count": 512,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Forward AD gradient step \n",
    "e = net_f(x_f, Wh_f[:], Wo_f[:], y_f);\n",
    "dnet_Wh(x, wh, wo, y) = J(w -> net_f(x, w, wo, y), wh);\n",
    "dWh_f[:] = dnet_Wh(x_f, Wh_f[:], Wo_f[:], y_f);\n",
    "dnet_Wo(x, wh, wo, y) = J(w -> net_f(x, wh, w, y), wo);\n",
    "dWo_f[:] = dnet_Wo(x_f, Wh_f[:], Wo_f[:], y_f);\n",
    "Wh_f -= 0.001dWh_f;\n",
    "Wo_f -= 0.001dWo_f;\n",
    "push!(e_f, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Vector{Float64}:\n",
       " 676.3729930074519\n",
       "   8.283530501772738\n",
       "   2.8080078728759292"
      ]
     },
     "execution_count": 515,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reverse AD gradient step\n",
    "e = forward!(graph);\n",
    "backward!(graph);\n",
    "Wh_r.output -= 0.001Wh_r.gradient;\n",
    "Wo_r.output -= 0.001Wo_r.gradient;\n",
    "push!(e_r, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.6206475198683705, 1.6206475198683645)"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_f = step_forward();\n",
    "loss_r = step_reverse();\n",
    "loss_f, loss_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.332 s (320850 allocations: 7.34 GiB)\n",
      "  32.250 μs (249 allocations: 113.48 KiB)\n"
     ]
    }
   ],
   "source": [
    "using BenchmarkTools\n",
    "function autodiff_forward()\n",
    "    dWh_f[:] = dnet_Wh(x_f, Wh_f[:], Wo_f[:], y_f);\n",
    "    dWo_f[:] = dnet_Wo(x_f, Wh_f[:], Wo_f[:], y_f);\n",
    "end\n",
    "\n",
    "function autodiff_reverse()\n",
    "    backward!(graph)\n",
    "end\n",
    "\n",
    "@btime autodiff_forward()\n",
    "@btime autodiff_reverse()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.2",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4bd624a0593993fe43ac4046b27b898fb2ef75c21c08f81e89e64ea0f51df676"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
